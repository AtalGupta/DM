# The Art of Image Generation

*By: Atal Gupta, Vinit Singh*  
*Date: June 25, 2025*  


<div style="display: flex; flex-direction: column; margin-bottom: 20px; align-items: center;">
  <div style="display: flex; justify-content: center; gap: 40px; margin-bottom: 10px;">
    <img src="assets/images/zoro.png" height="500px" width="400px" alt="Original reference image" />
    <img src="assets/images/generated.png" height="500px" width="400px" alt="AI-generated result" />
  </div>
  <p style="text-align: center; font-style: italic;">
    Left: Original reference image | Right: Image generated using OpenAI with the prompt:
    "
Photorealistic rendering of two samurai warriors standing together in vibrant traditional Japanese attire. One has spiky black hair, a straw hat on his back, and wears a bright red kimono with a yellow sash and blue shoulder armor, confidently smiling. The other has short green hair, three swords‚Äîone held in his mouth‚Äîand wears a white robe with a green and black haori, standing fiercely with a determined expression. Detailed facial features, realistic skin textures, accurate sword materials (polished steel blades with ornate hilts), high-resolution fabric textures, and natural lighting. The background is a clear blue sky with scattered clouds and distant rocky cliffs, mimicking traditional Japanese landscapes. Wide-angle cinematic perspective, ultra-realistic style."
  </p>
</div>

Have you ever wondered how artificial intelligence can generate images that never existed before? The answer lies in sophisticated mathematical frameworks built upon probability theory, combinatorics, and optimization. This blog explores the mathematical foundations of generative models, demonstrating how discrete mathematics principles enable machines to create realistic images.

Modern image generation represents one of the most striking applications of probabilistic modeling in computer science. Behind every AI-generated artwork lies a complex mathematical system that transforms random noise into structured visual content through learned probability distributions.

## Mathematical Background and Foundations

### Probability Theory Fundamentals

**Definition 1 (Probability Space):** A probability space is a triple (Œ©, F, P) where:
- Œ© is the sample space (set of all possible outcomes)
- F is a œÉ-algebra on Œ© (collection of events)  
- P: F ‚Üí [0,1] is a probability measure satisfying P(Œ©) = 1

For image generation, our sample space Œ© represents all possible digital images.

**Definition 2 (Joint Probability Distribution):** For random variables X‚ÇÅ, X‚ÇÇ, ..., X‚Çô, the joint probability distribution is:

```
P(X‚ÇÅ = x‚ÇÅ, X‚ÇÇ = x‚ÇÇ, ..., X‚Çô = x‚Çô) = P(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)
```

**Chain Rule of Probability:** For any sequence of random variables:

```
P(X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) = P(X‚ÇÅ) ¬∑ P(X‚ÇÇ|X‚ÇÅ) ¬∑ P(X‚ÇÉ|X‚ÇÅ,X‚ÇÇ) ¬∑ ... ¬∑ P(X‚Çô|X‚ÇÅ,...,X‚Çô‚Çã‚ÇÅ)
```

This fundamental rule becomes crucial in auto-regressive image generation models.

### Information Theory and Entropy

**Definition 3 (Shannon Entropy):** For a discrete random variable X with probability mass function P(X):

```
H(X) = -‚àë P(x) log‚ÇÇ P(x)
```

**Definition 4 (Kullback-Leibler Divergence):** The KL divergence between two probability distributions P and Q is:

```
D_KL(P||Q) = ‚àë P(x) log(P(x)/Q(x))
```

This measures how one probability distribution differs from another, essential for training generative models.

![Dr.png](assets/images/Dr.png)

## Data Representation

### Mathematical Formulation of Digital Images

![Image.png](assets/images/Image.png)



Let an image I be represented as a function:

```
I: {1, 2, ..., H} √ó {1, 2, ..., W} √ó {R, G, B} ‚Üí {0, 1, 2, ..., 255}
```

Where H and W are height and width in pixels.

For computational purposes, we flatten this to a vector:

```
x ‚àà ‚Ñù^(H√óW√ó3) where x ‚àà [0, 255]^(H√óW√ó3)
```

**Example:** A 256√ó256 RGB image becomes a vector in ‚Ñù^196,608.

### The Curse of Dimensionality in Image Space

**Problem Statement:** Given the image space ‚Ñù^d where d = H√óW√ó3, what is the probability of randomly generating a "natural" image?

**Analysis:**
- Total possible discrete images: 256^(H√óW√ó3)
- For a 64√ó64 RGB image: 256^12,288 ‚âà 10^29,810 possible images
- Estimated "natural" images: ‚â™ 10^1000 (still astronomically small relative to total space)

**Probability Calculation:**
```
P(natural image) = |Natural Images| / |Total Images| ‚âà 0
```

**Theorem 2 (Volume Concentration):** In high dimensions, data concentrates near the surface of hyperspheres, leaving most of the volume empty.

**Corollary:** Random sampling in high-dimensional image space has negligible probability of producing realistic images.

**Solution Approach:** Generative models learn to map from a lower-dimensional latent space Z ‚äÇ ‚Ñù^k (where k ‚â™ d) to the image space through a function G: ‚Ñù^k ‚Üí ‚Ñù^d.

This transforms the problem from sampling in ‚Ñù^d to sampling in the much more manageable ‚Ñù^k.

## Generative Models: Mathematical Frameworks

### Auto-regressive Models

**Mathematical Foundation:**

Auto-regressive models decompose the joint probability distribution of an image using the chain rule of probability.

**Definition 5 (Auto-regressive Decomposition):** For an image represented as a sequence of pixels x‚ÇÅ, x‚ÇÇ, ..., x‚Çô:

```
P(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) = ‚àè·µ¢‚Çå‚ÇÅ‚Åø P(x·µ¢ | x‚ÇÅ, x‚ÇÇ, ..., x·µ¢‚Çã‚ÇÅ)
```

**Computational Model:**
Each conditional probability P(x·µ¢ | x‚ÇÅ, ..., x·µ¢‚Çã‚ÇÅ) is parameterized by a neural network fŒ∏:

```
P(x·µ¢ = k | x‚ÇÅ, ..., x·µ¢‚Çã‚ÇÅ) = softmax(fŒ∏(x‚ÇÅ, ..., x·µ¢‚Çã‚ÇÅ))‚Çñ
```

where k ‚àà {0, 1, ..., 255} for pixel intensities.

**Training Objective:**
Maximize the log-likelihood:

```
L(Œ∏) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø log P(x·µ¢ | x‚ÇÅ, ..., x·µ¢‚Çã‚ÇÅ; Œ∏)
```

**Algorithmic Implementation:**
1. Define pixel ordering (typically raster scan: left-to-right, top-to-bottom)
2. For each position i, predict P(x·µ¢ | x<·µ¢) using neural network
3. Sample x·µ¢ ~ P(x·µ¢ | x<·µ¢) during generation
4. Repeat until complete image is generated

**Complexity Analysis:**
- Training: O(n¬∑T) where T is network evaluation time
- Generation: O(n¬∑T) (sequential, cannot be parallelized)

**Example - PixelCNN Architecture:**
For pixel at position (i,j), the conditional probability depends only on pixels in the "causal receptive field":

```
P(x·µ¢‚±º | x<(i,j)) where x<(i,j) = {x‚Çñ‚Çó : (k,l) precedes (i,j) in ordering}
```
Think of auto-regressive models as an artist who paints a picture one brushstroke at a time, where each stroke is informed by all previous strokes. The mathematical beauty lies in the chain rule decomposition‚Äîwe transform an intractable joint probability P(x‚ÇÅ,...,x‚Çô) into a product of manageable conditional probabilities.

In practice, if you're generating a 32√ó32 image (1,024 pixels), instead of modeling 256^1,024 possible images directly, you model 1,024 conditional distributions, each with 256 outcomes. This reduces computational complexity from exponential to linear in the number of pixels.

This approach guarantees that each pixel is conditioned only on previously generated pixels, maintaining the auto-regressive property.

### Variational Autoencoders (VAEs)

**Mathematical Foundation:**

VAEs combine variational inference with neural networks to learn latent representations of images.

**Problem Setup:**
- Observed data: x ‚àà ‚Ñù·µà (images)
- Latent variables: z ‚àà ‚Ñù·µè (where k ‚â™ d)
- Prior: p(z) = ùí©(0, I)
- Likelihood: pŒ∏(x|z) (decoder network)
- Posterior: qœÜ(z|x) (encoder network)

**Variational Lower Bound:**
The evidence lower bound (ELBO) is:

```
ELBO = ùîºqœÜ(z|x)[log pŒ∏(x|z)] - D_KL(qœÜ(z|x) || p(z))
```

**Theorem 3 (ELBO Derivation):**
For any variational distribution qœÜ(z|x):

```
log p(x) ‚â• ELBO = ‚à´ qœÜ(z|x) log[pŒ∏(x|z)p(z)/qœÜ(z|x)] dz
```

**Proof:**
Using Jensen's inequality on the concave logarithm function.

**Parameterization:**
- Encoder: qœÜ(z|x) = ùí©(ŒºœÜ(x), œÉ¬≤œÜ(x)I)
- Decoder: pŒ∏(x|z) = ùí©(ŒºŒ∏(z), œÉ¬≤I) or Bernoulli for binary images

**Reparameterization Trick:**
To enable backpropagation through stochastic layers:

```
z = ŒºœÜ(x) + œÉœÜ(x) ‚äô Œµ, where Œµ ~ ùí©(0, I)
```

**Training Algorithm:**
1. Forward pass: x ‚Üí (ŒºœÜ(x), œÉœÜ(x)) ‚Üí z ‚Üí ŒºŒ∏(z)
2. Compute ELBO loss
3. Backpropagate through reparameterized samples

**Generation Process:**
1. Sample z ~ ùí©(0, I)
2. Generate x = ŒºŒ∏(z)

VAEs solve a fundamental problem: how do we find a low-dimensional representation that captures the essence of high-dimensional data? The mathematical insight is profound‚Äîby forcing the latent representation to follow a known distribution (usually Gaussian), we create a continuous space where interpolation makes semantic sense.

The reparameterization trick is mathematically elegant: instead of sampling directly from qœÜ(z|x), we sample deterministic noise Œµ and transform it. This allows gradients to flow through the stochastic layer, enabling end-to-end training.

### Generative Adversarial Networks (GANs)

**Mathematical Foundation:**

GANs formulate image generation as a two-player zero-sum game between a generator and discriminator.

**Game-Theoretic Formulation:**

```
min_G max_D V(D,G) = ùîº_x~p_data[log D(x)] + ùîº_z~p_z[log(1 - D(G(z)))]
```

Where:
- G: ‚Ñù·µè ‚Üí ‚Ñù·µà (generator network)
- D: ‚Ñù·µà ‚Üí [0,1] (discriminator network)
- p_data: true data distribution
- p_z: prior noise distribution (typically ùí©(0,I))

**Theorem 4 (Global Optimum):** The global optimum of the GAN objective occurs when p_g = p_data, where p_g is the generator's distribution.

**Proof Sketch:**
At optimum, the discriminator cannot distinguish real from generated data: D*(x) = 1/2 for all x.

**Training Algorithm (Alternating Optimization):**
1. **Discriminator Update:**
   ```
   max_D [ùîº_x~p_data[log D(x)] + ùîº_z~p_z[log(1 - D(G(z)))]]
   ```

2. **Generator Update:**
   ```
   min_G ùîº_z~p_z[log(1 - D(G(z)))]
   ```
   (Often replaced with max_G ùîº_z~p_z[log D(G(z))] for better gradients)

**Convergence Analysis:**
The GAN training corresponds to minimizing the Jensen-Shannon divergence:

```
JSD(p_data || p_g) = 1/2 D_KL(p_data || M) + 1/2 D_KL(p_g || M)
```

where M = (p_data + p_g)/2

GANs formulate generation as a minimax game, drawing from game theory. The mathematical equilibrium occurs when the generator's distribution perfectly matches the data distribution. The discriminator acts as a learned distance metric, providing increasingly sophisticated feedback to the generator.

The key insight is that we're approximating the data distribution without explicitly modeling it‚Äîthe discriminator implicitly defines what "realistic" means through adversarial training.

### Diffusion Models

**Mathematical Foundation:**

![diffusion.png](assets/images/diffusion.png)

Diffusion models learn to reverse a gradual noising process through score-based generative modeling.

**Forward Process (Noise Addition):**
Define a Markov chain that gradually adds Gaussian noise:

```
q(x‚ÇÅ:T | x‚ÇÄ) = ‚àè·µó‚Çå‚ÇÅ·µÄ q(x‚Çú | x‚Çú‚Çã‚ÇÅ)
```

where:
```
q(x‚Çú | x‚Çú‚Çã‚ÇÅ) = ùí©(x‚Çú; ‚àö(1-Œ≤‚Çú) x‚Çú‚Çã‚ÇÅ, Œ≤‚ÇúI)
```

**Closed-form Forward Process:**
Using the reparameterization Œ±‚Çú = 1 - Œ≤‚Çú and ·æ±‚Çú = ‚àèÀ¢‚Çå‚ÇÅ·µó Œ±‚Çõ:

```
q(x‚Çú | x‚ÇÄ) = ùí©(x‚Çú; ‚àö·æ±‚Çú x‚ÇÄ, (1-·æ±‚Çú)I)
```

**Reverse Process (Denoising):**
Learn to reverse the forward process:

```
pŒ∏(x‚ÇÄ:T) = p(xT) ‚àè·µó‚Çå‚ÇÅ·µÄ pŒ∏(x‚Çú‚Çã‚ÇÅ | x‚Çú)
```

where:
```
pŒ∏(x‚Çú‚Çã‚ÇÅ | x‚Çú) = ùí©(x‚Çú‚Çã‚ÇÅ; ŒºŒ∏(x‚Çú,t), Œ£Œ∏(x‚Çú,t))
```

**Training Objective:**
Minimize the variational lower bound:

```
L = ùîºq[‚àë·µó‚Çå‚ÇÅ·µÄ D_KL(q(x‚Çú‚Çã‚ÇÅ|x‚Çú,x‚ÇÄ) || pŒ∏(x‚Çú‚Çã‚ÇÅ|x‚Çú))]
```

**Simplified Training Loss:**
The loss reduces to:

```
L_simple = ùîºt,x‚ÇÄ,Œµ [||Œµ - ŒµŒ∏(‚àö·æ±‚Çú x‚ÇÄ + ‚àö(1-·æ±‚Çú) Œµ, t)||¬≤]
```

where ŒµŒ∏ is a neural network that predicts the noise Œµ ~ ùí©(0,I).

![Maths.png](assets/images/Maths.png)

**Generation Algorithm:**
1. Sample xT ~ ùí©(0,I)
2. For t = T, T-1, ..., 1:
   ```
   x‚Çú‚Çã‚ÇÅ = 1/‚àöŒ±‚Çú (x‚Çú - Œ≤‚Çú/‚àö(1-·æ±‚Çú) ŒµŒ∏(x‚Çú,t)) + œÉ‚Çúz
   ```
   where z ~ ùí©(0,I) and œÉ‚Çú is the noise schedule.

**Score-based Interpretation:**
The noise prediction ŒµŒ∏ is related to the score function:

```
‚àáx log q(x‚Çú) = -ŒµŒ∏(x‚Çú,t)/‚àö(1-·æ±‚Çú)
```

This connects diffusion models to score-based generative modeling theory.

Diffusion models exploit a deep connection to thermodynamics and stochastic processes. The forward process increases entropy (adds noise), while the reverse process decreases entropy (removes noise). The mathematical framework connects to Langevin dynamics and score-based modeling.

The remarkable property is that by learning to predict noise at each timestep, the model implicitly learns the score function ‚àá‚Çì log p(x), which fully characterizes the data distribution.


### Comparative Analysis

| Model Type | Mathematical Foundation | Strengths | Computational Complexity |
|------------|------------------------|-----------|-------------------------|
| Auto-regressive | Chain rule of probability | Exact likelihood, stable training | O(n) generation time |
| VAE | Variational inference | Fast generation, interpretable latent space | O(1) generation time |
| GAN | Game theory, adversarial training | High-quality samples | O(1) generation time |
| Diffusion | Stochastic processes, score matching | State-of-the-art quality | O(T) generation time |


## Conclusion

This exploration demonstrates how fundamental mathematical concepts from discrete mathematics, probability theory, and optimization enable sophisticated image generation capabilities. The key insights are:

1. **Probabilistic Modeling:** All successful generative models learn to approximate the underlying data distribution through different mathematical frameworks.

2. **Dimensionality Reduction:** The curse of dimensionality necessitates learning lower-dimensional representations or structured dependencies.

3. **Mathematical Rigor:** Each model class addresses the generation problem through rigorous mathematical formulations‚Äîchain rule decomposition, variational inference, game theory, and stochastic processes.

4. **Computational Trade-offs:** Different mathematical approaches lead to different computational characteristics, affecting training stability, generation speed, and sample quality.

The mathematics of generative models illustrates how abstract mathematical concepts translate into practical applications that are transforming creative industries. As computational resources continue to grow, these mathematical foundations will enable even more sophisticated approaches to artificial creativity.

For undergraduate students, this field represents an excellent intersection of theoretical mathematics and practical applications, demonstrating the power of probability theory, optimization, and discrete mathematics in solving real-world problems.

## References

1. Goodfellow, I., et al. (2014). "Generative Adversarial Networks." *Advances in Neural Information Processing Systems*. arXiv:1406.2661.

2. Kingma, D. P., & Welling, M. (2013). "Auto-Encoding Variational Bayes." *International Conference on Learning Representations*. arXiv:1312.6114.

3. Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *Advances in Neural Information Processing Systems*. arXiv:2006.11239.

4. van den Oord, A., et al. (2016). "Pixel Recurrent Neural Networks." *International Conference on Machine Learning*. arXiv:1601.06759.

5. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.

6. Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

7. Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). "Variational Inference: A Review for Statisticians." *Journal of the American Statistical Association*, 112(518), 859-877.

8. Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *Advances in Neural Information Processing Systems*. arXiv:1907.05600.

9. Dhariwal, P., & Nichol, A. (2021). "Diffusion Models Beat GANs on Image Synthesis." *Advances in Neural Information Processing Systems*. arXiv:2105.05233.

10. Razavi, A., et al. (2019). "Generating Diverse High-Fidelity Images with VQ-VAE-2." *Advances in Neural Information Processing Systems*. arXiv:1906.00446.